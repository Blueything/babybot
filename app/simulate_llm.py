import requests

def simulate_llm_response(prompt: str) -> dict:
    print("âš™ï¸ Sending prompt to LLM:", prompt)

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3",
                "prompt": prompt,
                "stream": False
            },
            timeout=15  # ğŸ•’ max wait time in seconds
        )
        data = response.json()["response"].strip()
        print("âœ… LLM Response:", data)

        return {
            "message": data,
            "tips": [line.strip("-â€¢ ") for line in data.split("\n") if line.strip()]
        }

    except Exception as e:
        print("âŒ LLM Error:", e)
        return {
            "message": "âš ï¸ LLM unavailable.",
            "tips": ["Make sure Ollama is running."]
        }
